################################################################################
# Dockerfile that builds 'yanwk/sd-webui-base:bulky'.
# 
# A environment not only for running AUTOMATIC1111/stable-diffusion-webui,
# but also includes all compile tools for Magma, PyTorch & xFormers.
# 
# Building this image could timeout on GitHub Actions, so better build it locally.
# Image size could be ~8GiB.
#
# Beware, check the version numbers before build!
################################################################################

FROM opensuse/leap:15.4 AS downloading

LABEL maintainer="code@yanwk.fun"

# It's not best practice to use so much layers.
# But in this case, we don't want a big huge layer that won't be cached.
# Plus, we need to define several stages to utilize 'docker build --target' to divide jobs.

RUN set -eu \
    && mkdir -p /root/wheels

RUN --mount=type=cache,target=/var/cache/zypp \
    zypper install -y \
        git make cmake ninja find \
        fish shadow plocate aria2 \
        gperftools-devel libgthread-2_0-0 Mesa-libGL1 \
        python310 python310-pip python310-devel 

RUN cd /usr/bin \
    && ln -sf python3 python \
    && ln -sf python3.10 python3 \
    && ln -sf pydoc3.10 pydoc3

# Use TCMALLOC from gperftools.
ENV LD_PRELOAD=libtcmalloc.so

# https://gitlab.com/nvidia/container-images/cuda/-/tree/master/dist
# https://www.intel.com/content/www/us/en/developer/tools/oneapi/onemkl-download.html
RUN printf "\
[cuda-opensuse15-x86_64]\n\
name=cuda-opensuse15-x86_64\n\
baseurl=https://developer.download.nvidia.com/compute/cuda/repos/opensuse15/x86_64\n\
enabled=1\n\
gpgcheck=1\n\
gpgkey=https://developer.download.nvidia.com/compute/cuda/repos/opensuse15/x86_64/D42D0685.pub\n" \
        > /etc/zypp/repos.d/cuda-opensuse15.repo \
    && printf "\
[intel-oneapi]\n\
name=intel-oneapi\n\
baseurl=https://yum.repos.intel.com/oneapi\n\
enabled=1\n\
gpgcheck=1\n\
gpgkey=https://yum.repos.intel.com/intel-gpg-keys/GPG-PUB-KEY-INTEL-SW-PRODUCTS.PUB\n" \
        > /etc/zypp/repos.d/intel-oneapi.repo

RUN --mount=type=cache,target=/var/cache/zypp \
    zypper --gpg-auto-import-keys \
        install --no-confirm --no-recommends --auto-agree-with-licenses \
            intel-oneapi-compiler-dpcpp-cpp intel-oneapi-compiler-fortran \
            intel-oneapi-mkl intel-oneapi-mkl-devel

RUN --mount=type=cache,target=/var/cache/zypp \
    zypper --gpg-auto-import-keys \
        install --no-confirm --no-recommends --auto-agree-with-licenses \
            cuda-cudart-11-8 cuda-compat-11-8 \
            cuda-libraries-11-8 cuda-nvtx-11-8 libnpp-11-8 libcublas-11-8 \
            cuda-command-line-tools-11-8 cuda-libraries-devel-11-8 \
            cuda-minimal-build-11-8 cuda-cudart-devel-11-8 cuda-nvprof-11-8 \
            cuda-nvml-devel-11-8 libcublas-devel-11-8 libnpp-devel-11-8 \
            cuda-cccl-11-8 cuda-nvcc-11-8 cuda-nvrtc-devel-11-8 

# PATH for NVCC
ENV PATH="${PATH}:/usr/local/cuda-11.8/bin"
ENV LD_LIBRARY_PATH="${LD_LIBRARY_PATH}:/usr/local/cuda-11.8/lib64"

FROM downloading AS building

RUN --mount=type=cache,target=/root/.cache/pip \
    pip install ninja wheel setuptools numpy \
    && pip install --pre torch torchvision --force-reinstall \
        --index-url https://download.pytorch.org/whl/nightly/cu118 

# Compile-install xformers
# Reduce build-targets to save time on compiling!
# https://github.com/facebookresearch/xformers/blob/main/README.md#install-troubleshooting
# https://arnon.dk/matching-sm-architectures-arch-and-gencode-for-various-nvidia-cards/

# If targets only "6.1", it takes ~5min to compile, on a 8-Core (Ryzen 1700) CPU.
# If targets "6.1;7.5;8.0;8.6", it takes ~30min.
RUN --mount=type=cache,target=/root/.cache/pip \
    pip install -r https://raw.githubusercontent.com/facebookresearch/xformers/main/requirements.txt

RUN cd /root \
    && git clone --depth=1 --recurse-submodules --shallow-submodules \
        https://github.com/facebookresearch/xformers.git 

ENV TORCH_CUDA_ARCH_LIST="6.1+PTX;7.5;8.0;8.6;8.9"

RUN --mount=type=cache,target=/root/.cache/pip \
    --mount=type=cache,target=/root/xformers/build \
    --mount=type=cache,target=/root/xformers/dist \
    cd /root/xformers \
    && python setup.py bdist_wheel -d /root/wheels

# 'build' folder is mounted as cache and cache won't be included in the final image.
# So there's no need to keep source folder.
# Besides, we keep wheel file for possible further use. It's a long compile after all :-D
RUN --mount=type=cache,target=/root/.cache/pip \
    rm -rf /root/xformers \
    && pip install /root/wheels/*.whl

# All remaining deps are described in txt
COPY ["requirements.txt","/root/"]
RUN --mount=type=cache,target=/root/.cache/pip \
    pip install -r /root/requirements.txt

FROM building AS running

# Fix for TensorRT
WORKDIR /usr/lib/python3.10/site-packages/tensorrt
RUN ln -s libnvinfer_plugin.so.8 libnvinfer_plugin.so.7 \
    && ln -s libnvinfer.so.8 libnvinfer.so.7
ENV LD_LIBRARY_PATH="${LD_LIBRARY_PATH}:/usr/lib/python3.10/site-packages/tensorrt"

# Create a low-privilege user.
RUN sed -i 's/^CREATE_MAIL_SPOOL=yes/CREATE_MAIL_SPOOL=no/' /etc/default/useradd \
    && mkdir -p /home/runner /home/scripts \
    && groupadd runner \
    && useradd runner -g runner -d /home/runner \
    && chown runner:runner /home/runner /home/scripts

COPY --chown=runner:runner scripts/. /home/scripts/

USER runner:runner
VOLUME /home/runner
WORKDIR /home/runner
ENV CLI_ARGS=""
EXPOSE 7860
STOPSIGNAL SIGINT
CMD bash /home/scripts/entrypoint.sh
